{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load TensorBoard Notebook Extension\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 14:  Ising Phases using Convolutional Neural Networks in PyTorch "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Goal\n",
    "The primary goal of this notebooks is to learn how to implement a Convolutional Neural Network (CNN) using the powerful PyTorch package. It also introduces core concepts for CNNs such as convolutional and pooling layers and padding.\n",
    "\n",
    "## Overview\n",
    "In this notebook, we will write a simple convolutional neural network (CNN) in Pytorch for classifying phases for the Ising Model. We will consider perhaps the simplest CNN: a single convolutional layer with depth $N \\in \\{1,5,10,20,50\\}$. This will introduce the power of the Pytorch framework to make dynamic graphs. \n",
    "\n",
    "We will use this notebook to characterize samples drawn from the 2D Ising model at various temperatures. This is the same dataset that has been used in all earlier examples. Recall that the critical temperature for the Ising model is $T_c=2.26$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x21f1a790890>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "import os,sys\n",
    "import numpy as np\n",
    "import torch # pytorch package, allows using GPUs\n",
    "# fix seed\n",
    "seed=17\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structure of the Procedure\n",
    "\n",
    "Constructing a Deep Neural Network to solve ML problems is a multiple-stage process. Quite generally, one can identify the key steps as follows:\n",
    "\n",
    "* ***step 1:*** Load and process the data\n",
    "* ***step 2:*** Define the model and its architecture\n",
    "* ***step 3:*** Choose the optimizer and the cost function\n",
    "* ***step 4:*** Train the model \n",
    "* ***step 5:*** Evaluate the model performance on the *unseen* test data\n",
    "* ***step 6:*** Modify the hyperparameters to optimize performance for the specific data set\n",
    "\n",
    "Below, we sometimes combine some of these steps together for convenience."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Load and Process the Ising Dataset\n",
    "\n",
    "We start by defining the dataset class for Pytorch.\n",
    "\n",
    "We have three types of samples in the Ising dataset: samples drawn from deep in the disordered phase, samples drawn from the ordered phase, and samples drawn from near the critical phase which we do not use for training. The goal is to classify whether a sample comes from $T>T_c$ or $T<T_c$.\n",
    "\n",
    "There is standard way to load data when using the PyTorch package, which we discussed in the DNN example for the SUSY dataset. Here, we just switch to the 2D-Ising data instead.\n",
    "\n",
    "To proceed, download the Ising dataset and insert the proper `path_to_data` in the script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets # load data\n",
    "\n",
    "class Ising_Dataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Ising pytorch dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, data_type, transform=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data_type (string): `train`, `test` or `critical`: creates data_loader\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        from sklearn.model_selection import train_test_split\n",
    "        import collections\n",
    "        import pickle as pickle\n",
    "\n",
    "\n",
    "        L=40 # linear system size\n",
    "        T=np.linspace(0.25,4.0,16) # temperatures\n",
    "        T_c=2.26 # critical temperature in the TD limit\n",
    "\n",
    "        # path to data directory\n",
    "        path_to_data = r'C:\\\\Users\\\\apaar\\\\Downloads\\\\'\n",
    "        \n",
    "        file_name = r'Ising2DFM_reSample_L40_T=All_labels.pkl' # this file contains 16*10000 samples taken in T=np.arange(0.25,4.0001,0.25)\n",
    "        data = pickle.load(open(path_to_data+file_name,'rb')) # pickle reads the file and returns the Python object (1D array, compressed bits)\n",
    "        data = np.unpackbits(data).reshape(-1, 1600) # Decompress array and reshape for convenience\n",
    "        data=data.astype('int')\n",
    "        data[np.where(data==0)]=-1 # map 0 state to -1 (Ising variable can take values +/-1)\n",
    "\n",
    "        file_name = r'Ising2DFM_reSample_L40_T=All_labels.pkl' # this file contains 16*10000 samples taken in T=np.arange(0.25,4.0001,0.25)\n",
    "        labels = pickle.load(open(path_to_data+file_name,'rb')) # pickle reads the file and returns the Python object (here just a 1D array with the binary labels)\n",
    "\n",
    "        # divide data into ordered, critical and disordered\n",
    "\n",
    "        X_ordered=data[:70000,:]\n",
    "        Y_ordered=labels[:70000]\n",
    "\n",
    "        X_critical=data[70000:100000,:]\n",
    "        Y_critical=labels[70000:100000]\n",
    "\n",
    "        X_disordered=data[100000:,:]\n",
    "        Y_disordered=labels[100000:]\n",
    "\n",
    "        del data,labels\n",
    "        # define training, critical and test data sets\n",
    "        X=np.concatenate((X_ordered,X_disordered)) #np.concatenate((X_ordered,X_critical,X_disordered))\n",
    "        Y=np.concatenate((Y_ordered,Y_disordered)) #np.concatenate((Y_ordered,Y_critical,Y_disordered))\n",
    "\n",
    "        # pick random data points from ordered and disordered states to create the training and test sets\n",
    "        X_train,X_test,Y_train,Y_test=train_test_split(X,Y,test_size=0.2,train_size=0.8)\n",
    "\n",
    "\n",
    "        if data_type=='train':\n",
    "            X=X_train\n",
    "            Y=Y_train\n",
    "            print(\"Training on 80 percent of examples\")\n",
    "\n",
    "        if data_type=='test':\n",
    "            X=X_test\n",
    "            Y=Y_test\n",
    "            print(\"Testing on 20 percent of examples\")\n",
    "\n",
    "        if data_type=='critical':\n",
    "            X=X_critical\n",
    "            Y=Y_critical\n",
    "            print(\"Predicting on %i critical examples\"%len(Y_critical))\n",
    "\n",
    "        # reshape data back to original 2D-array form\n",
    "        X=X.reshape(X.shape[0],40,40)\n",
    "\n",
    "        # these are necessary attributes in dataset class and must be assigned\n",
    "        self.data=(X,Y)\n",
    "        self.transform = transform\n",
    "\n",
    "\n",
    "    # override __len__ and __getitem__ of the Dataset() class\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data[1])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        sample=(self.data[0][idx,...],self.data[1][idx])\n",
    "        if self.transform:\n",
    "            sample=self.transform(sample)\n",
    "\n",
    "        return sample\n",
    "\n",
    "    \n",
    "def load_data(kwargs):\n",
    "    # kwargs:  CUDA arguments, if enabled\n",
    "    # load and noralise train,test, and data\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        Ising_Dataset(data_type='train'),\n",
    "        batch_size=args.batch_size, shuffle=True, **kwargs)\n",
    "\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        Ising_Dataset(data_type='test'),\n",
    "        batch_size=args.test_batch_size, shuffle=True, **kwargs)\n",
    "\n",
    "    critical_loader = torch.utils.data.DataLoader(\n",
    "        Ising_Dataset(data_type='critical'),\n",
    "        batch_size=args.test_batch_size, shuffle=True, **kwargs)\n",
    "\n",
    "    return train_loader, test_loader, critical_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Define the Neural Net and its Architecture\n",
    "\n",
    "Similar to the discussion in the SUSY DNN notebook, we then define the architecture of the neural net in the `model` class which contains the `forward` function method that tells us how to produce the output given some input. The backpropagaiton algorithm is implemented automatically by the Pytorch package.\n",
    "\n",
    "Recall that a CNN is composed of convolutional layers, max-pool layer, often followed by a fully connected layer and then the classifier. In the architecture below, we start with a convolutional layer that takes as an input a layer with $D_{in}=1$ with height and width $H=W=40$, a receptive field or filter size of $2 \\times 2$, and depth $N$ (there are $N$ layers). We also add a padding of zeros on both sides of the image.  This convolutional layer can be summarized by the four numbers $[N,D_{in},H,W]=[N,1,41,41]$. This is then fed into a $2 \\times 2$ maxpool layer which results in layer of size $[N,1,20,20]$. This layer is then hooked up to a linear layer that takes as an `[input,output]` of the form `[N*20*20*1,2]` since there are $2$ classes (corresponding to the ordered and disordered phases). We use a logistic (softmax) classifier as the output and train using various optimizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn # construct NN\n",
    "\n",
    "class model(nn.Module):\n",
    "    # create convolutional net\n",
    "    def __init__(self, N=10, L=40):\n",
    "        # inherit attributes and methods of nn.Module\n",
    "        super(model, self).__init__()\t\n",
    "        # create convolutional layer with input depth 1 and output depth N\n",
    "        self.conv1 = nn.Conv2d(1, N, kernel_size=2, padding=1)\n",
    "        # batch norm layer takes Depth\n",
    "        self.bn1=nn.BatchNorm2d(N) \n",
    "        # create fully connected layer after maxpool operation reduced 40->18\n",
    "        self.fc1 = nn.Linear(20*20*N, 2) \t\n",
    "        self.N=N\n",
    "        self.L=L\n",
    "        print(\"The number of neurons in CNN layer is %i\"%(N))\n",
    "\n",
    "    def forward(self, x):\n",
    "        #Unsqueeze command indicates one channel and turns x.shape from (:,40,40) to (:,1, 40,40)\n",
    "        x=F.relu(self.conv1(torch.unsqueeze(x,1).float()))\n",
    "        #print(x.shape)  often useful to look at shapes for debugging\n",
    "        x = F.max_pool2d(x,2)\t \n",
    "        #print(x.shape)\n",
    "        x=self.bn1(x) # largely unnecessary and here just for pedagogical purposes\n",
    "        return F.log_softmax(self.fc1(x.view(-1,20*20*self.N)), dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the `train` and `test` functions\n",
    "\n",
    "These are very standard functions for going over data to train and evaluate the model. \n",
    "\n",
    "Since we will be testing the CNN performance on both the test and the critical data, the `test` function accepts two arguments: `data_loader` and `verbose` to allow control over the input data and the printing messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "import tensorflow as tf\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# adding tensor board\n",
    "logdir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)\n",
    "\n",
    "def train(epoch):\n",
    "    # these are very standard functions for going over data to train\n",
    "\n",
    "    CNN.train() # effects Dropout and BatchNorm layers\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        if args.cuda:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        output = CNN(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "def test(data_loader,verbose='Test'):\n",
    "    # these are very standard functions for evaluating data\n",
    "\n",
    "    CNN.eval() # effects Dropout and BatchNorm layers\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in data_loader:\n",
    "        if args.cuda:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        output = CNN(data)\n",
    "        test_loss += F.nll_loss(output, target, size_average=False).item() # sum up batch loss\n",
    "        pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum().item()\n",
    "\n",
    "    test_loss /= len(data_loader.dataset)\n",
    "    print('\\n'+verbose+' set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(data_loader.dataset),\n",
    "        100. * correct / len(data_loader.dataset)))\n",
    "    accuracy=100. * correct / len(data_loader.dataset)\n",
    "    return(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Model Parameters\n",
    "\n",
    "Next we define the training settings. This proceeds in the same way as in Notebook 13 on the SUSY dataset, except we now also show how to turn on the `cuda` library option of PyTorch which enables parallel coputations (whenever resources for this are available). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse # handles arguments\n",
    "import sys; sys.argv=['']; del sys # required to use parser in jupyter notebooks\n",
    "\n",
    "# training settings\n",
    "parser = argparse.ArgumentParser(description='PyTorch Convmodel Ising Example')\n",
    "parser.add_argument('--batch-size', type=int, default=64, metavar='N',\n",
    "                    help='input batch size for training (default: 64)')\n",
    "parser.add_argument('--test-batch-size', type=int, default=1000, metavar='N',\n",
    "                    help='input batch size for testing (default: 1000)')\n",
    "parser.add_argument('--epochs', type=int, default=10, metavar='N',\n",
    "                    help='number of epochs to train (default: 10)')\n",
    "parser.add_argument('--lr', type=float, default=0.01, metavar='LR',\n",
    "                    help='learning rate (default: 0.01)')\n",
    "parser.add_argument('--momentum', type=float, default=0.5, metavar='M',\n",
    "                    help='SGD momentum (default: 0.5)')\n",
    "parser.add_argument('--no-cuda', action='store_true', default=False,\n",
    "                    help='disables CUDA training')\n",
    "parser.add_argument('--seed', type=int, default=1, metavar='S',\n",
    "                    help='random seed (default: 1)')\n",
    "parser.add_argument('--log-interval', type=int, default=10, metavar='N',\n",
    "                    help='how many batches to wait before logging training status')\n",
    "args = parser.parse_args()\n",
    "args.epochs=5\n",
    "args.cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "\n",
    "torch.manual_seed(args.seed)\n",
    "if args.cuda:\n",
    "    torch.cuda.manual_seed(args.seed)\n",
    "\n",
    "cuda_kwargs = {'num_workers': 1, 'pin_memory': True} if args.cuda else {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps 3+4+5: Choose the Optimizer and the Cost Function. Train and Evaluate the Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Expected an input array of unsigned byte data type",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m writer \u001b[38;5;241m=\u001b[39m SummaryWriter(logdir)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# load data\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m train_loader, test_loader, critical_loader\u001b[38;5;241m=\u001b[39m\u001b[43mload_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcuda_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m test_array\u001b[38;5;241m=\u001b[39m[]\n\u001b[0;32m      8\u001b[0m critical_array\u001b[38;5;241m=\u001b[39m[]\n",
      "Input \u001b[1;32mIn [2]\u001b[0m, in \u001b[0;36mload_data\u001b[1;34m(kwargs)\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_data\u001b[39m(kwargs):\n\u001b[0;32m     93\u001b[0m     \u001b[38;5;66;03m# kwargs:  CUDA arguments, if enabled\u001b[39;00m\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;66;03m# load and noralise train,test, and data\u001b[39;00m\n\u001b[0;32m     95\u001b[0m     train_loader \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataLoader(\n\u001b[1;32m---> 96\u001b[0m         \u001b[43mIsing_Dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m,\n\u001b[0;32m     97\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     99\u001b[0m     test_loader \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataLoader(\n\u001b[0;32m    100\u001b[0m         Ising_Dataset(data_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[0;32m    101\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mtest_batch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    103\u001b[0m     critical_loader \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataLoader(\n\u001b[0;32m    104\u001b[0m         Ising_Dataset(data_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcritical\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[0;32m    105\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mtest_batch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "Input \u001b[1;32mIn [2]\u001b[0m, in \u001b[0;36mIsing_Dataset.__init__\u001b[1;34m(self, data_type, transform)\u001b[0m\n\u001b[0;32m     26\u001b[0m file_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIsing2DFM_reSample_L40_T=All_labels.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;66;03m# this file contains 16*10000 samples taken in T=np.arange(0.25,4.0001,0.25)\u001b[39;00m\n\u001b[0;32m     27\u001b[0m data \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;28mopen\u001b[39m(path_to_data\u001b[38;5;241m+\u001b[39mfile_name,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m)) \u001b[38;5;66;03m# pickle reads the file and returns the Python object (1D array, compressed bits)\u001b[39;00m\n\u001b[1;32m---> 28\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munpackbits\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1600\u001b[39m) \u001b[38;5;66;03m# Decompress array and reshape for convenience\u001b[39;00m\n\u001b[0;32m     29\u001b[0m data\u001b[38;5;241m=\u001b[39mdata\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mint\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     30\u001b[0m data[np\u001b[38;5;241m.\u001b[39mwhere(data\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m0\u001b[39m)]\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;66;03m# map 0 state to -1 (Ising variable can take values +/-1)\u001b[39;00m\n",
      "File \u001b[1;32m<__array_function__ internals>:5\u001b[0m, in \u001b[0;36munpackbits\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Expected an input array of unsigned byte data type"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F # implements forward and backward definitions of an autograd operation\n",
    "import torch.optim as optim # different update rules such as SGD, Nesterov-SGD, Adam, RMSProp, etc\n",
    "writer = SummaryWriter(logdir)\n",
    "# load data\n",
    "train_loader, test_loader, critical_loader=load_data(cuda_kwargs)\n",
    "\n",
    "test_array=[]\n",
    "critical_array=[]\n",
    "\n",
    "# create array of depth of convolutional layer\n",
    "N_array=[1,5,10,20,50]\n",
    "\n",
    "# loop over depths\n",
    "for N in N_array:\n",
    "    CNN = model(N=N)\n",
    "    if args.cuda:\n",
    "        CNN.cuda()\n",
    "\n",
    "    # negative log-likelihood (nll) loss for training: takes class labels NOT one-hot vectors!\n",
    "    criterion = F.nll_loss\n",
    "    # define SGD optimizer\n",
    "    optimizer = optim.SGD(CNN.parameters(), lr=args.lr, momentum=args.momentum)\n",
    "    #optimizer = optim.Adam(DNN.parameters(), lr=0.001, betas=(0.9, 0.999))\n",
    "\n",
    "    # train the CNN and test its performance at each epoch\n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        train(epoch)\n",
    "        if epoch==args.epochs:\n",
    "            test_array.append(test(test_loader,verbose='Test'))\n",
    "            critical_array.append(test(critical_loader,verbose='Critical'))\n",
    "        else:\n",
    "            test(test_loader,verbose='Test')\n",
    "            test(critical_loader,verbose='Critical')\n",
    "    print(test_array)\n",
    "    print(critical_array)\n",
    "    \n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Accuracy\n",
    "\n",
    "Let us now plot the accuracy of our calculation. Notice that even with a convolutional layer of depth 1 (one set of weights and a single bias!) we can get a 100% accuracy on the test set. We do less well on the critical data (somewhere between 80-90%) with lots of fluctuations from training run to training to run. Again, this shows you the incredible power and (some of the limitations) of all these ML methods. If the dataset we care about (critical region) is not exactly the dataset we train on, our accuracy can be significantly diminished."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "## Print the result for different N\n",
    "%matplotlib inline\n",
    "\n",
    "plt.plot(N_array, test_array, 'r-*', label=\"test\")\n",
    "plt.plot(N_array, critical_array, 'b-s', label=\"critical\")\n",
    "plt.ylim(60,110)\n",
    "plt.xlabel('Depth of hidden layer', fontsize=24)\n",
    "plt.xticks(N_array)\n",
    "plt.ylabel('Accuracy', fontsize=24)\n",
    "plt.legend(loc='best', fontsize=24)\n",
    "plt.tick_params(axis='both', which='major', labelsize=24)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "* Do __Step 6__: modify the hyperparameters to optimize performance for the specific data set\n",
    "* The strides used above do not account for the periodic boundary conditions. Define your own stride function in PyTorch to incorporate periodic boundary conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
